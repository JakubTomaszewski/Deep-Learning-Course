{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zadanie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tym razem zadanie polega na stworzeniu klasyfikatora obrazków działającego na 50 klasach z różnymi przedmiotami/zwierzętami itp. Do dyspozycji mają Państwo zbiór treningowy podzielony na odpowiednie podfoldery z klasami oraz zbiór testowy - bez podziału. Zbiór treningowy jest przygotowany w taki sposób by można go było łatwo załadować za pomocą klasy torchvision.ImageFolder wywołując np:\n",
    "trainset = ImageFolder(\"data/train/\", transform=train_transform)\n",
    "Wówczas wszystkie przykłady zostaną przypisane do odpowiedniej klasy w zależności od tego w jakim podfolderze się znajdowały.\n",
    "Jako że dane są bardzo duże to umieściłem je na OneDrive:\n",
    "(train.zip i test_all.zip)\n",
    "\n",
    "Proszę zwrócić szczególną uwagę na formę zwracanego rozwiązania, bo ostatnio większość z państwa zrobiła to byle jak i miałem bardzo dużo problemów z dodawaniem/usuwaniem niepotrzebnych wierszy itp. Tym razem nie będę poprawiał przesyłanych przez Państwa plików!\n",
    "W ramach rozwiązania, proszę oddać poprzez teamsy plik archiwum .zip z kodem (w formie notebooka, lub skryptu/skryptów .py) oraz plikiem .csv z predykcjami na zbiorze testowym. BEZ dodatkowych podfolderów i BEZ danych. W ramach predykcji proszę zapisać tym razem dwie kolumny (bez nagłówków):\n",
    "- Pierwszą kolumnę z nazwami plików testowych (uwaga pliki nazywają się np. 850043533599308.JPEG a nie 850043533599308.jpeg, 850043533599308.jpg czy 850043533599308). Proszę zwrócić na to uwagę bo mój skrypt ewaluacyjny inaczej nie zadziała.\n",
    "- Drugą kolumnę z wartościami oznaczającą predykcję numeru klasy. Klasy ponumerowane są zgodnie z numeracją ze zbioru treningowego (startując od zera). Po utworzeniu datasetu mogą to państwo sprawdzić wywołując trainset.classes.\n",
    "\n",
    "Uwaga: W zadaniu proszę nie wykorzystywać gotowych architektur o których wspominałem na zajęciach, poświęcimy temu zagadnieniu całe ćwiczenia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing libraries and modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import csv\n",
    "import natsort\n",
    "\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from torchvision.models import resnet18\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize, RandomRotation, RandomAffine, RandomErasing, Grayscale, RandomHorizontalFlip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping():\n",
    "    def __init__(self, tolerance=5, min_delta=0):\n",
    "\n",
    "        self.tolerance = tolerance\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, train_loss, validation_loss):\n",
    "        if (validation_loss - train_loss) > self.min_delta:\n",
    "            self.counter +=1\n",
    "            if self.counter >= self.tolerance:  \n",
    "                self.early_stop = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    \"\"\"Returns the available device for computation.\n",
    "    Returns:\n",
    "        torch.device: available device for computation\n",
    "    \"\"\"\n",
    "    compute_device = None\n",
    "    if torch.cuda.is_available():\n",
    "        compute_device = torch.device('cuda')\n",
    "    elif torch.backends.mps.is_available():\n",
    "        compute_device = torch.device('mps')\n",
    "    else:\n",
    "        compute_device = torch.device('cpu')\n",
    "    \n",
    "    print(f'device is {compute_device}')\n",
    "    return compute_device\n",
    "\n",
    "\n",
    "def save_predictions_to_csv(predictions, file_path):\n",
    "    with open(file_path, \"w\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerows(predictions.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Params and constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 100\n",
    "device = get_device()\n",
    "# device = torch.device('cpu')\n",
    "\n",
    "train_data_path = 'train/'\n",
    "test_data_opath = 'test_all/'\n",
    "confusion_matrix_path = 'conf_matrix.png'\n",
    "\n",
    "ROTATION_ANGLE = 15\n",
    "RANDOM_AFFINE = 20\n",
    "VAL_SET_FRACTION = 0.15\n",
    "CROP_FACTOR = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(random_seed)\n",
    "np.random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_transformations = Compose([\n",
    "    ToTensor(),\n",
    "    Grayscale(),\n",
    "    Normalize((0.5), (0.5)),\n",
    "    RandomHorizontalFlip(0.2),\n",
    "    RandomErasing(0.2),\n",
    "    RandomRotation(ROTATION_ANGLE),\n",
    "    RandomAffine(RANDOM_AFFINE)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dataset = ImageFolder(train_data_path, transform=img_transformations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names, class_idx = img_dataset.find_classes(train_data_path)\n",
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(class_names)\n",
    "num_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data to train and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VAL_SET_SIZE = int(len(img_dataset) * VAL_SET_FRACTION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, val_dataset = torch.utils.data.random_split(img_dataset, [len(img_dataset) - VAL_SET_SIZE, VAL_SET_SIZE])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Train set size: {len(train_dataset)}')\n",
    "print(f'Val set size: {len(val_dataset)}')\n",
    "\n",
    "assert sum([len(train_dataset), len(val_dataset)]) == len(img_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=512, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_counts_dataset_full = Counter(img_dataset.targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_class_distribution(class_labels, class_counts, class_names, label):\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.bar(class_labels, class_counts)\n",
    "\n",
    "    plt.title(label)\n",
    "    plt.ylabel('Count')\n",
    "    plt.xlabel('Class')\n",
    "    plt.xticks(range(num_classes), class_names, rotation=90)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_class_distribution(class_counts_dataset_full.keys(), class_counts_dataset_full.values(), class_names, label='Class distribution for the whole dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some imbalances in the class labels for `bread` and `carbon` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "sample_images, sample_labels = iter(train_dataloader).next()\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "imshow(make_grid(sample_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = train_dataset[0][0].shape\n",
    "input_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationModelTemplate(nn.Module):\n",
    "    def __init__(self, num_classes, device, callbacks) -> None:\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.device = device\n",
    "        self.callbacks = callbacks\n",
    "        self.train_report = {\n",
    "            'train_loss_history': [],\n",
    "            'val_loss_history': []\n",
    "        }\n",
    "\n",
    "    def forward(self, X):\n",
    "        return X\n",
    "    \n",
    "    def predict(self, X):\n",
    "        self.eval()\n",
    "        y_pred = self.forward(X)\n",
    "        return torch.tensor([torch.argmax(pred) for pred in y_pred])\n",
    "    \n",
    "    def fit(self, train_dataloader, loss_func, optimizer, epochs, val_dataloader=None):\n",
    "        self.train()\n",
    "        for epoch in range(epochs):\n",
    "            print('================================')\n",
    "            print(f'Epoch {epoch}')\n",
    "            train_epoch_loss = self._train_one_epoch(train_dataloader, loss_func, optimizer)\n",
    "            avg_epoch_loss = train_epoch_loss / len(train_dataloader)\n",
    "            self._log_training_loss(avg_epoch_loss, epoch)\n",
    "            \n",
    "            if val_dataloader is not None:\n",
    "                val_loss = self.perform_validation(loss_func, val_dataloader)\n",
    "                avg_val_loss = val_loss / len(val_dataloader)\n",
    "                self._log_validation_loss(avg_val_loss)\n",
    "            \n",
    "            if self.callbacks is not None:\n",
    "                if self.callbacks.get['early_stopping'] is not None:\n",
    "                    self.callbacks['early_stopping'](avg_epoch_loss, avg_val_loss)\n",
    "                    if self.callbacks['early_stopping'].early_stop:\n",
    "                        print('Stopping early')\n",
    "                        break\n",
    "    \n",
    "    def _train_one_epoch(self, train_dataloader, loss_func, optimizer):\n",
    "        epoch_loss = 0.0\n",
    "        for batch_samples, batch_labels in tqdm(train_dataloader):\n",
    "            X = batch_samples.to(self.device)\n",
    "            y = batch_labels.to(self.device)\n",
    "            \n",
    "            y_pred = self.forward(X)\n",
    "            loss = loss_func(y_pred, y)\n",
    "            self._optimize_params(loss, optimizer)\n",
    "            epoch_loss += loss.item()\n",
    "        return epoch_loss\n",
    "            \n",
    "    def _optimize_params(self, loss, optimizer):\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    def perform_validation(self, loss_func, val_dataloader):\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch_samples, batch_labels in val_dataloader:\n",
    "                X = batch_samples.to(self.device)\n",
    "                y = batch_labels.to(self.device)\n",
    "                \n",
    "                y_pred = self.forward(X)\n",
    "                loss = loss_func(y_pred, y)\n",
    "                val_loss += loss.item()\n",
    "        return val_loss\n",
    "    \n",
    "    def _log_training_loss(self, epoch_loss, epoch):\n",
    "        self.train_report['train_loss_history'].append(epoch_loss)\n",
    "        print(f'Train Loss: {epoch_loss}')\n",
    "    \n",
    "    def _log_validation_loss(self, epoch_loss):\n",
    "        self.train_report['val_loss_history'].append(epoch_loss)\n",
    "        print(f'Val Loss: {epoch_loss}')\n",
    "    \n",
    "    def get_conf_matrix(self, dataloader):\n",
    "        confusion_matrix = torch.zeros(self.num_classes, self.num_classes)\n",
    "        with torch.no_grad():\n",
    "            for batch_samples, batch_labels in dataloader:\n",
    "                X = batch_samples.to(self.device)\n",
    "                y = batch_labels.to(self.device)\n",
    "                \n",
    "                y_pred = self.predict(X)\n",
    "                for t, p in zip(y.view(-1), y_pred.view(-1)):\n",
    "                        confusion_matrix[t.long(), p.long()] += 1\n",
    "        return confusion_matrix\n",
    "    \n",
    "    def class_accuracy(self, dataloader):\n",
    "        conf_matrix = self.get_conf_matrix(dataloader)\n",
    "        return conf_matrix.diag()/conf_matrix.sum(1)\n",
    "\n",
    "\n",
    "class ClassificationModel(ClassificationModelTemplate):\n",
    "    def __init__(self, input_channels, num_classes, device, callbacks: dict=None) -> None:\n",
    "        super().__init__(num_classes, device, callbacks)\n",
    "        \n",
    "        self.conv_pooling_stack = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, 64, kernel_size=5, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.Dropout(0.6),\n",
    "            nn.MaxPool2d(2, 2), # 32x32x16\n",
    "            nn.Conv2d(64, 128, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.Dropout(0.6),\n",
    "            nn.MaxPool2d(2, 2), #15x15x32\n",
    "            nn.Conv2d(128, 256, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.6),\n",
    "            nn.MaxPool2d(2, 2) #6x6x64\n",
    "        )\n",
    "        self.flatten = nn.Flatten(1)\n",
    "        \n",
    "        self.fully_connected_stack = nn.Sequential(\n",
    "            nn.Linear(256*6*6, 2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.6),\n",
    "            nn.Linear(2048, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.6),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.6),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.output_layer = nn.Linear(64, self.num_classes)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        X = self.conv_pooling_stack(X)\n",
    "        X = self.flatten(X)\n",
    "        X = self.fully_connected_stack(X)\n",
    "        return self.output_layer(X)\n",
    "    \n",
    "\n",
    "# Transfer learning for tests\n",
    "class ResNetPretrained(ClassificationModelTemplate):\n",
    "    def __init__(self, num_classes, device, callbacks: dict=None) -> None:\n",
    "        super().__init__(num_classes, device, callbacks)\n",
    "        self.resnet_model = resnet18(pretrained=True)\n",
    "        num_features = self.resnet_model.fc.in_features\n",
    "        self.resnet_model.fc = nn.Linear(num_features, num_classes)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        return self.resnet_model(X)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        self.resnet_model.eval()\n",
    "        y_pred = self.forward(X)\n",
    "        return torch.tensor([torch.argmax(pred) for pred in y_pred])\n",
    "    \n",
    "    def parameters(self):\n",
    "        return self.resnet_model.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://paperswithcode.com/sota/incremental-learning-on-cifar-100-50-classes-2\n",
    "\n",
    "\n",
    "# ResNet for performance testing\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, input_channels, out_channels, stride=1, downsample = None):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "                        nn.Conv2d(input_channels, out_channels, kernel_size=3, stride=stride, padding=1),\n",
    "                        nn.BatchNorm2d(out_channels),\n",
    "                        nn.ReLU())\n",
    "        \n",
    "        self.conv2 = nn.Sequential(\n",
    "                        nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
    "                        nn.BatchNorm2d(out_channels))\n",
    "        \n",
    "        self.downsample = downsample\n",
    "        self.relu = nn.ReLU()\n",
    "        self.out_channels = out_channels\n",
    "        \n",
    "    def forward(self, X):\n",
    "        residual = X\n",
    "        out = self.conv1(X)\n",
    "        out = self.conv2(out)\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(X)\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNetImplementation(ClassificationModelTemplate):\n",
    "    def __init__(self, residual_block, layers, num_classes, device, callbacks: dict=None) -> None:\n",
    "        super().__init__(num_classes, device, callbacks)\n",
    "        self.inplanes = 64\n",
    "        \n",
    "        self.conv1 = nn.Sequential(\n",
    "                        nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3),\n",
    "                        nn.BatchNorm2d(64),\n",
    "                        nn.ReLU())\n",
    "        \n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        self.residual_block_stack = nn.Sequential(\n",
    "            self._make_layer(residual_block, 64, layers[0], stride=1),\n",
    "            self._make_layer(residual_block, 128, layers[1], stride=2),\n",
    "            self._make_layer(residual_block, 256, layers[2], stride=2),\n",
    "            self._make_layer(residual_block, 512, layers[3], stride=2)\n",
    "        )\n",
    "\n",
    "        self.avgpool = nn.AvgPool2d(2, stride=1)\n",
    "        self.output_layer = nn.Linear(512, num_classes)\n",
    "    \n",
    "    def _make_layer(self, residual_block, planes, residual_blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes:\n",
    "            \n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes, kernel_size=1, stride=stride),\n",
    "                nn.BatchNorm2d(planes),\n",
    "            )\n",
    "        layers = []\n",
    "        layers.append(residual_block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes\n",
    "        for _ in range(1, residual_blocks):\n",
    "            layers.append(residual_block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        X = self.conv1(X)\n",
    "        X = self.maxpool(X)\n",
    "        X = self.residual_block_stack(X)\n",
    "\n",
    "        X = self.avgpool(X)\n",
    "        X = X.view(X.size(0), -1)\n",
    "        return self.output_layer(X)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        self.eval()\n",
    "        y_pred = self.forward(X)\n",
    "        return torch.tensor([torch.argmax(pred) for pred in y_pred])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(5, 0.3)\n",
    "\n",
    "callbacks = {'early_stopping': early_stopping}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = ResNetClassificationModel(num_classes, device).to(device)\n",
    "# model = ResNetImplementation(ResidualBlock, [3, 4, 6, 3], num_classes, device).to(device)\n",
    "model = ClassificationModel(1, num_classes, device).to(device)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_dataloader, loss_func, optimizer, epochs=35, val_dataloader=val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(model.train_report['train_loss_history'], label='Train')\n",
    "plt.plot(model.train_report['val_loss_history'], label='Validation')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy per class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc_per_class = model.class_accuracy(train_dataloader)\n",
    "val_acc_per_class = model.class_accuracy(val_dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Train acc: {torch.mean(train_acc_per_class)}')\n",
    "print(f'Val acc: {torch.mean(val_acc_per_class)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataSet(Dataset):\n",
    "    def __init__(self, main_dir, transform):\n",
    "        self.main_dir = main_dir\n",
    "        self.transform = transform\n",
    "        self.images = natsort.natsorted(os.listdir(main_dir))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_loc = os.path.join(self.main_dir, self.images[idx])\n",
    "        image = Image.open(img_loc).convert(\"RGB\")\n",
    "        tensor_image = self.transform(image)\n",
    "        return tensor_image\n",
    "    \n",
    "    def getImageName(self, idx):\n",
    "        return self.images[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_for_test_dataset(test_dataset, test_dataloader, model):\n",
    "    with torch.no_grad():\n",
    "        predictions = []\n",
    "        for batch in test_dataloader:\n",
    "            predictions.append(model.predict(batch.to(model.device)))\n",
    "        predictions_flat = [prediction.item() for sublist in predictions for prediction in sublist]\n",
    "    return dict(zip(test_dataset.images, predictions_flat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = TestDataSet(test_data_opath, transform=img_transformations)\n",
    "test_dataloader = DataLoader(test_dataset , batch_size=24, shuffle=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predict_for_test_dataset(test_dataset, test_dataloader, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_predictions_to_csv(predictions, 'poniedzialek_Kulesza_Tomaszewski_results.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('SSNE')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bd6aaa34b2eaa27615c35a49f49fcac2b9fb6ef67dd9795ab23618a0054bdc5d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
